[TOC]

### 基础知识

Hadoop生态系统是为处理大数据集而产生的一个合乎成本效益的解决方案。Hadoop实现了一个特别的计算模型，也就是MapReduce，其可以将计算任务分割成多个处理单元然后分散到一群家用的或服务器级别的硬件机器上，从而降低成本并提供水平可伸缩性。这个计算模型的下面是一个被称为Hadoop分布式文件系统（HDFS）的分布式文件系统。这个文件系统是“可插拔的”，而且现在已经出现了几个商用的和开源的替代方案。

Hive最适合于数据仓库应用程序，使用该应用程序进行相关的静态数据分析，不需要快速响应给出结果，而且数据本身不会频繁变化。

#### MapReduce综述

MapReduce是一种计算模型，该模型可将大型数据处理任务分解成很多个单个的、可以在服务器集群中并行执行的任务。这些任务的计算结果可以合并在一起来计算最终的结果。 

MapReduce这个术语来自于两个基本的数据转换操作：map过程和reduce过程。一个map操作会将集合中的元素从一种形式转换成另一种形式。在这种情况下，输入的键-值对会被转换成零到多个键-值对输出。其中，输入和输出的键必须完全不同，而输入和输出的值则可能完全不同。

在MapReduce计算框架中，某个键的所有键-值对都会被分发到同一个reduce操作中。确切地说，这个键和这个键所对应的所有值都会被传递给同一个Reducer。Reduce过程的目的是将值的集合转换成一个值（例如对一组数值求和或求平均值），或者转换成另一个集合。这个Reducer最终会产生一个键-值对。再次说明一下，输入和输出的键和值可能是不同的。需要说明的是，如果job不需要reduce过程的话，那么也是可以无reduce过程的。

Hadoop提供了一套基础设施来处理大多数困难的工作以保证任务能够执行成功。例如，Hadoop决定如果将提交的job分解成多个独立的map和reduce任务（task）来执行，它就会对这些task进行调度并为其分配合适的资源，决定将某个task分配到集群中哪个位置（如果可能，通常是这个task所要处理的数据所在的位置，）这样可以最小化网络开销。它会监控每一个task以确保其成功完成，并重启一些失败的task。

Hadoop分布式文件系统（也就是HDFS），或者一个同类的分布式文件系统，管理着集群中的数据。每个数据块（block）都会被冗余多份（通常默认会冗余3份），这样可以保证不会因单个硬盘或服务器的损坏导致数据丢失。同时，因为其目标是优化处理非常大的数据集，所以HDFS以及类似的文件系统所使用的数据块都非常大，通常是64MB或是这个值的若干倍。这么大的数据块可以在硬盘上连续进行存储，这样可以保证以最少的磁盘寻址次数来进行写入和读取，从而最大化提高读写性能。 

#### Hadoop生态系统中的Hive

有好几种方式可以与Hive进行交互，一般我们主要关注于CLI，也就是命令行界面。对于那些更喜欢图形用户界面的用户，可以使用现在逐步出现的商业和开源的解决方案，如：Karmasphere，Cloudera提供的开源的Hue项目，以及Qubole提供的“Hive即服务”方式，等。

Hive发行版中附带的模块有CLI，一个称为Hive网页界面（HWI）的简单网页界面，以及可通过JDBC、ODBC和一个Thrift服务器进行编程访问的几个模块。
所有的命令和查询都会进入到Driver（驱动模块），通过该模块对输入进行解析编译，对需求的计算进行优化，然后按照指定的步骤执行（通常是启动多个MapReduce任务（job）来执行）。当需要启动MapReduce任务（job）时，Hive本身是不会生成Java MapReduce算法程序的。相反，Hive通过一个表示“job执行计划”的XML文件驱动执行内置的、原生的Mapper和Reducer模块。换句话说，这些通用的模块函数类似于微型的语言翻译程序，而这个驱动计算的“语言”是以XML形式编码的。

Hive通过和JobTracker通信来初始化MapReduce任务（job），而不必部署在JobTracker所在的管理节点上执行。在大型集群中，通常会有网关机专门用于部署像Hive这样的工具。在这些网关机上可远程和管理节点上的JobTracker通信来执行任务（job）。通常，要处理的数据文件是存储在HDFS中的，而HDFS是由NameNode进行管理的。

Metastore（元数据存储）是一个独立的关系型数据库（通常是一个MySQL实例），Hive会在其中保存表模式和其他系统元数据。


#### Java和Hive：词频统计算法

##### Java

```java


```

##### Hive

```SQL
CREATE TABLE DOCS(line STIRNG);

LOAD DATA INPATH 'docs' OVERWRITE INTO TABLE docs;

CREATE TABLE word_counts AS
SELECT word,count(1) AS count 
FROM (SELECT explode(split(line,'\s')) AS word FROM docs) w
GROUP BY word
order by word
;
```

### 基础操作

#### 本地模式

因为Hive中大多数工作是使用Hadoop的job，所有Hive的行为可以反映出用户所使用的Hadoop运行模式。不过，即使在分布式模式下执行，Hive还是可以在提交查询前判断是否可以使用本地模式来执行这个查询。这时它会读取数据文件，然后自己管理MapReduce task，最终提供更快的执行方式。不过，对于Hadoop来说，不同模式之间的差异相对于部署方式更多地在于执行方式上。

当处理小数据集时，使用本地模式执行可以使Hive执行得更快些。设置如下属性set hive.exec.mode.local.auto=true;时，将会触发Hive更主动地使用这种模式，即使当前用户是在分布式模式或伪分布式模式下执行Hadoop的。如果想默认使用这个配置，可以将这个命令加到$HOME/.hiverc文件中。

### 数据类型和文件格式

### HiveQL:数据定义

### HiveQL:数据操作

### HiveQL:数据查询

### HiveQL:视图

### HiveQL:索引

### 调优

### 开发

### 函数

### 模式设计

### Issues

宽表，列数太多：[HIVE-11592](https://issues.apache.org/jira/browse/HIVE-11592)